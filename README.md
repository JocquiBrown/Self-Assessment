# Self-Assessment Deliverable

## Self-Assessment
For our project I volunteered early on to work on developing the database for our project. This included loading the data into Jupyter notebook. Creating a Pandas dataframe. Cleaning the data, transforming the data, and finally exporting the dataframes as csv files. Outside of Jupyter Notebook I divided our data into three broad categories based on the questions the respondents were asked about themselves and their health habits. I determined that the best way to divide the data was to create one table for each category (demographic information, morbidity data, and health metrics). I then created an ERD (entity relationship diagram) for our dataset and created the schema for our database. To build the database, I used PostgreSQL to create the tables and then imported the data into the tables. Then the tables were merged back into one table before being integrated into the machine learning model. Using AWS and psycopg2 in Jupyter notebook I worked in tandem with a group member to complete the integration. 

The greatest personal challenge I had when completing the project was integrating the database with the machine learning model. When attempting to integrate the database we ran into several error messages in Jupyter Notebook. Luckily, we were able to overcome this challenge when asking one of the TSAs to look over our code.  When they determined that our code looked fine, we eventually concluded that there was an issue with our database in RDS and then found out that it was unable to connect to the database created in Postgres because the database not set to “public” in AWS. This turned out to be an easy fix as all we had to do was change the settings in RDS and the database was able to successfully be integrated with our machine learning model. 

As mentioned, I worked with my group member who was tasked with developing the machine learning model by working with him to integrate the database with the machine learning model. I also worked with another group member on visualizations by suggesting that we analyze diabetes and then I went on to present the results of that analysis during our presentation. Finally, I worked with my final teammate by adding images to the presentation that they primarily created using Google Slides. 

## Project and Team Summary
To communicate we decided as a group to reach out to one another, via text, by creating a group chat. This allowed us to send updates to each other instantly to an application we all check frequently. 

The primary strength of our team was the ability of each team member to communicate effectively and make their ideas and intentions clear for the project. This allowed us to avoid wasting time working on something that someone else was working on or waiting on something to be completed without having an update on the status of that particular task. A few suggestions I would have for a new cohort kicking off the project include, establishing a method of communication, creating an outline of the questions they hope to answer in relation to their chosen topic, and setting goals and expectations for each group member, early on, so that everyone is cognizant of the expectations of the project as well as what specific tasks they will be carrying out. Adherence to these suggestions will make task completion much smoother when working on the project. 

For our group we quickly divided the work evenly amongst the group members based on overarching objectives. One member was assigned each of the following creation of the database, building the machine learning model, and creating visualizations to analyze trends within the dataset. When deciding on a topic we quickly noted that the group had a common interest in health care and with the leading cause of death in the United States being heart disease we felt it was a natural fit to address this topic for our project. 

When creating the database, we used Python’s Pandas library to clean and transform the data, we then loaded the data into PostgresSQL and used AWS to integrate the database with the machine learning model. For our machine learning model, we used a random forest algorithm to sample the data. A random forest algorithm uses several decision trees, which classify the data using a series of True/False statements. Then the random forest algorithm combines these decision tree outputs to sort each object into a class. Lastly, we used Tableau to visualize our data. We specifically looked at the roles that race, hours of sleep per night, body mass index, sex, and diabetes played in the likelihood of having heart disease. 

Overall, the results of the random forest model were pretty good. The accuracy of the model was 75% and the recall and precision scores were 0.91 and 0.78 respectively for predicting True Positives. However, the recall and precision scores were much lower, 0.35 and 0.61 respectively for predicting True Negatives. This means that our model is significantly better at correctly identifying respondents with heart disease as opposed to those without the condition. For the trends we noticed when analyzing the data, we noticed that diabetic respondents were more likely to report having heart disease than their non-diabetic counterparts. We also noticed that heart disease was more common among male respondents when compared to their female counterparts and we also noticed that white respondents were more likely to report having heart disease. However, BMI and hours of sleep didn’t appear to be correlated with heart disease based on our dataset. 
 

